{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zdXDwHEfRW2f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Get api key: secret(left side bar top 4) -> Gemini API keys (unavailable in HK, use vpn) as env: GOOGLE_API_KEY\n",
        "'''\n",
        "!curl https://ipinfo.io/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3a6tSo1Msr8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%bash\n",
        "#install packages\n",
        "\n",
        "#gemini\n",
        "pip install llama-index-multi-modal-llms-gemini\n",
        "pip install llama-index-vector-stores-qdrant\n",
        "pip install llama-index-embeddings-gemini\n",
        "pip install llama-index-llms-gemini\n",
        "\n",
        "pip install llama-index 'google-generativeai>=0.3.0' matplotlib qdrant_client\n",
        "\n",
        "#hugging face\n",
        "pip install -U datasets datasets[vision]\n",
        "\n",
        "#image processing\n",
        "pip install Pillow #==11.2.1\n",
        "\n",
        "#colbert: tpu/gpu\n",
        "pip install gitpython\n",
        "pip install faiss-cpu\n",
        "\n",
        "pip install pandas gdown shortuuid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LigpmPka2N2w"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "'''\n",
        "setup colbert / plaidrepro\n",
        "'''\n",
        "\n",
        "!rm -rf ./ColBERT ./plaidrepro\n",
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "\n",
        "#some error using plaidrepro\n",
        "#!git -C ColBERT/ pull || git clone https://github.com/seanmacavaney/plaidrepro.git\n",
        "\n",
        "#!mv ./plaidrepro ./ColBERT\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n",
        "\n",
        "try: # When on google Colab, let's install all dependencies with pip.\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"If you're running outside Colab, please make sure you install ColBERT in conda following the instructions in our README. You can also install (as above) with pip but it may install slower or less stable faiss or torch dependencies. Conda is recommended.\")\n",
        "    assert False\n",
        "\n",
        "!pip install bitarray datasets gitpython ninja scipy spacy tqdm transformers ujson flask python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C5WaKsqbWZbW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "setup api keys\n",
        "'''\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "gemini = Gemini(model_name=\"models/gemma-3-27b-it\")\n",
        "response = gemini.chat(messages=[\n",
        "    ChatMessage(role=\"user\", content=\"Hello! What's your name?\")\n",
        "])\n",
        "print(response.message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdjouDApVc27"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "'''\n",
        "load benchmark data\n",
        "'''\n",
        "\n",
        "from datasets import load_dataset\n",
        "mrag_bench = load_dataset(\"uclanlp/MRAG-Bench\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD1JEynFau9k"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Wrapper for Gemini, with rate limit\n",
        "\n",
        "step-1, stage-1: LVM to describe images\n",
        "Refernce: https://docs.llamaindex.ai/en/stable/examples/multi_modal/gemini/\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import PIL\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core.llms import ChatMessage, ImageBlock\n",
        "from google.genai.types import HttpOptions\n",
        "\n",
        "class LVM():\n",
        "  def __init__(self, model_name, rate_limit=10):\n",
        "    self._llm = Gemini(model_name=model_name)\n",
        "    self._rate_limit = rate_limit   # Stores the minimum time (in seconds) to wait between API calls\n",
        "\n",
        "  # ------------------ Private Methods ------------------\n",
        "\n",
        "  def _inference(self, messages):\n",
        "      import time\n",
        "      res=None\n",
        "      fail_count=0\n",
        "      while True:\n",
        "        try:\n",
        "          time.sleep(self._rate_limit + 10*fail_count)\n",
        "          return self._llm.chat(messages=messages).message.content\n",
        "        except Exception as e:\n",
        "          fail_count += 1\n",
        "          print(f'fail[{fail_count}]: {e}')\n",
        "\n",
        "  def _attatch_multiple_images(self, images:list[ImageBlock], message: ChatMessage):\n",
        "    for img in images:\n",
        "      message.blocks.append(img)\n",
        "    return message\n",
        "\n",
        "  def _attatch(self, images:list[ImageBlock], messages: list[ChatMessage]):\n",
        "    for img, msg in zip(images, messages):\n",
        "      msg.blocks.append(img)\n",
        "    return messages\n",
        "\n",
        "  def _dataset_image_to_imageblock(self, images):\n",
        "    from io import BytesIO\n",
        "    from llama_index.core.llms import ImageBlock\n",
        "\n",
        "    imgs = []\n",
        "    for img in images:\n",
        "      buf = BytesIO()\n",
        "      img.save(buf, format=\"PNG\")\n",
        "      imgs.append(ImageBlock(image=buf.getvalue(), image_mimetype=\"image/png\"))\n",
        "\n",
        "    return imgs\n",
        "\n",
        "  # ------------------ Public Methods ------------------\n",
        "\n",
        "  def images_to_text(self, images: list[PIL.Image],\n",
        "      prompt=\"What is inside the pictures? Give short keywords description in 15 words, separated by comma. Response directly, no need to show you understand this prompt.\"\n",
        "  ):\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    images=self._dataset_image_to_imageblock(images)\n",
        "\n",
        "    messages = [ChatMessage(prompt) for _ in range(len(images))]\n",
        "    messages = self._attatch(images,messages)\n",
        "\n",
        "    return [\n",
        "      self._inference(messages=[msg])\n",
        "      for msg in tqdm(messages)\n",
        "    ]\n",
        "\n",
        "  def multi_images_chat(self, images: list[PIL.Image], prompt: str):\n",
        "    images=self._dataset_image_to_imageblock(images)\n",
        "\n",
        "    message = ChatMessage(prompt)\n",
        "    message = self._attatch_multiple_images(images, message)\n",
        "    return self._inference(messages=[message])\n",
        "\n",
        "\n",
        "#-------------------------------[example usage]-----------------------------\n",
        "#-------------------------------<images_to_text>---------------------------\n",
        "# For model selection reference: https://ai.google.dev/gemini-api/docs/rate-limits#free-tier\n",
        "#lvm = LVM(model_name=\"models/gemini-1.5-flash\")\n",
        "lvm = LVM(model_name=\"models/gemini-2.0-flash-lite\", rate_limit=5)\n",
        "\n",
        "images = mrag_bench[-2:]['image']\n",
        "messages = lvm.images_to_text(images=images)\n",
        "\n",
        "display(messages)\n",
        "print(f'length: {len(messages)}')\n",
        "\n",
        "#-------------------------------[example usage]-----------------------------\n",
        "#-------------------------------<multi_images_chat>---------------------------\n",
        "images = mrag_bench[-2:]['image']\n",
        "for img in images:\n",
        "  display(img)\n",
        "message = lvm.multi_images_chat(images=images,\n",
        "  prompt=\"What is inside each photo? What are the connections between all photos? Answer in short.\"\n",
        ")\n",
        "\n",
        "print(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlWC84bJ9S5_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "for storing/loading (image_filename, description) pairs in json format\n",
        "'''\n",
        "class ImageDescriptionStorage():\n",
        "  def __init__(self, path='./data.json', data=[]):\n",
        "    self._path = path\n",
        "    self._data = data\n",
        "\n",
        "  def load(self):\n",
        "    import json\n",
        "\n",
        "    try:\n",
        "      with open(self._path, 'r') as f:\n",
        "        self._data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "      self._data = [] # Initialize with empty list if file not found\n",
        "    return self._data\n",
        "\n",
        "  def append(self, dict_obj):\n",
        "    self._data.append(dict_obj)\n",
        "\n",
        "  def save(self):\n",
        "    import json\n",
        "\n",
        "    with open(self._path, 'w') as f:\n",
        "      json.dump(self._data, f, indent=4)\n",
        "\n",
        "  def get_data(self):\n",
        "    return self._data\n",
        "\n",
        "#------------------------[Example usage]---------------------------\n",
        "test_data=[\n",
        "    {\n",
        "        'filename': 'cat.jpg',\n",
        "        'description': 'A cat is dancing.'\n",
        "    },\n",
        "    {\n",
        "        'filename': 'dog.jpg',\n",
        "        'description': 'A dog is swimming.'\n",
        "    }\n",
        "]\n",
        "\n",
        "image_description_storage = ImageDescriptionStorage(path='./test_images_description.json')\n",
        "for d in test_data:\n",
        "  image_description_storage.append(d)\n",
        "image_description_storage.save()\n",
        "\n",
        "retrieve=image_description_storage.load()\n",
        "display(retrieve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JbVIMnojdjG4"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "#Download the pre-trained ColBERTv2 checkpoint\n",
        "\n",
        "rm -rf colbertv\n",
        "mkdir -p colbertv\n",
        "cd colbertv\n",
        "#wget -qq https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -O colbertv2.0.tar.gz\n",
        "wget -qq -O- https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz | tar xvz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fYe02JS_yH9t"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "step-1b: RAG system: Plaid\n",
        "'''\n",
        "\n",
        "class RAG():\n",
        "  '''\n",
        "    root_dir: for Colbertv working dir\n",
        "    data_path: for storing input data array, which is missing from Colbertv\n",
        "  '''\n",
        "  def __init__(self, root_dir='./experiments', storage_path='./rag_data.pt'):\n",
        "    import os\n",
        "\n",
        "    self._root_dir = root_dir\n",
        "    self._storage_path = storage_path\n",
        "\n",
        "    if os.path.exists(self._storage_path):\n",
        "        print(f\"Loading data from {self._storage_path}\")\n",
        "        self._load_storage()\n",
        "    else:\n",
        "        print(f\"Data file {self._storage_path} not found. Starting with empty data.\")\n",
        "        self._storage = {\n",
        "            #image descriptions\n",
        "            'data': [],\n",
        "            #filenames\n",
        "            'annotation': []\n",
        "        }\n",
        "\n",
        "  def _save_storage(self):\n",
        "    import json\n",
        "    with open(self._storage_path, 'w') as f:\n",
        "      json.dump(self._storage, f, indent=4)\n",
        "\n",
        "  def _load_storage(self):\n",
        "    import json\n",
        "    with open(self._storage_path, 'r') as f:\n",
        "      self._storage = json.load(f)\n",
        "\n",
        "  '''\n",
        "    Warning:\n",
        "      data list should be at least 100 to make it work, otherwise it stucks\n",
        "  '''\n",
        "  def index(self, data: list[str], annotation: list[str], kmeans_niters=4,\n",
        "    checkpoint_dir=\"./colbertv/colbertv2.0\"\n",
        "  ):\n",
        "    from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "    from colbert.data import Queries, Collection\n",
        "    from colbert import Indexer, Searcher\n",
        "\n",
        "    with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n",
        "        config = ColBERTConfig(\n",
        "            nbits=2,\n",
        "            root=self._root_dir,\n",
        "            kmeans_niters=kmeans_niters,\n",
        "            avoid_fork_if_possible=True\n",
        "        )\n",
        "        indexer = Indexer(checkpoint=checkpoint_dir, config=config)\n",
        "        indexer.index(name=\"msmarco.nbits.2\", collection=data, overwrite=True)\n",
        "\n",
        "    self._storage['data'] = data\n",
        "    self._storage['annotation'] = annotation\n",
        "    self._save_storage()\n",
        "\n",
        "    print('overwritten storage file!')\n",
        "\n",
        "  def query(self, query: str, top_k=5):\n",
        "    from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "    from colbert.data import Queries, Collection\n",
        "    from colbert import Indexer, Searcher\n",
        "\n",
        "    with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n",
        "      config = ColBERTConfig(\n",
        "          root=self._root_dir,\n",
        "      )\n",
        "      searcher = Searcher(index=\"msmarco.nbits.2\", config=config)\n",
        "      ranking = searcher.search(query, k=top_k)\n",
        "\n",
        "    return {\n",
        "      'index': ranking[0],\n",
        "      'data': [ self._storage['data'][passage_id] for passage_id in ranking[0] ],\n",
        "      'annotation': [ self._storage['annotation'][passage_id] for passage_id in ranking[0] ]\n",
        "    }\n",
        "\n",
        "#---------------------------[Example usage]----------------------------------\n",
        "\n",
        "test_data=[\n",
        "  f'Person_{i} has {i+3} cats!\\n'\n",
        "    for i in range(100)\n",
        "]\n",
        "test_annotation=[\n",
        "  f'image_{i}.png'\n",
        "    for i in range(len(test_data))\n",
        "]\n",
        "\n",
        "print('--------------------------use new rag------------------------------------')\n",
        "\n",
        "!rm -f ./test_rag_data.json\n",
        "\n",
        "rag = RAG(\n",
        "  storage_path='./test_rag_data.json'\n",
        ")\n",
        "\n",
        "query='Who has 11 cats?'\n",
        "\n",
        "rag.index(data=test_data, annotation=test_annotation)\n",
        "answer = rag.query(query=query)\n",
        "print(f'query: {query}')\n",
        "print(f'top-k answer: {answer}')\n",
        "print(f'best answer: {answer[\"data\"][0]} @{answer[\"annotation\"][0]}')\n",
        "\n",
        "print('--------------------------use old rag------------------------------------')\n",
        "rag = RAG(\n",
        "  storage_path='./test_rag_data.json'\n",
        ")\n",
        "\n",
        "query='Who has 3 cats?'\n",
        "\n",
        "answer = rag.query(query=query)\n",
        "print(f'query: {query}')\n",
        "print(f'top-k answer: {answer}')\n",
        "print(f'best answer: {answer[\"data\"][0]} @{answer[\"annotation\"][0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmZupWZvmdzW"
      },
      "outputs": [],
      "source": [
        "########################[BELOW TODO]################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M95ZBOqi_FeP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%bash\n",
        "#download the dataset to feed into database of RAG system\n",
        "rm -rf ./image_corpus ./mrag_bench_image_corpus\n",
        "\n",
        "gdown 1atwkNXH3aEtCLuqimZoB1Mifj5CwL3CL\n",
        "unzip mrag_bench_image_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EdfRm32x_XbD"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "#get small samples from image_corpus during developing/debug\n",
        "#may want to use all image when deploy\n",
        "\n",
        "rm -rf test_images_database\n",
        "mkdir test_images_database\n",
        "rm -f test_images_description.json test_rag_data.json\n",
        "\n",
        "(cd image_corpus && ls | grep -v input | shuf --random-source=<(yes 'i like cat') | head -n 10 | xargs -I{} cp '{}' ../test_images_database/)\n",
        "ls test_images_database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "97hCInlu_45O"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Merge Step-1:\n",
        "\n",
        "Input: folders of images\n",
        "Output: None\n",
        "Side Effect:\n",
        "  1. RAG stored the indexed descriptsion.\n",
        "  2. rag_storage_path has the filename, description pairs\n",
        "\n",
        "Steps:\n",
        "1. append_to_description_file: use LVM to make descitpion and output to .json file\n",
        "2. rag_indexing: use RAG to index the .json file\n",
        "'''\n",
        "\n",
        "class Pipeline_GenerateDescriptions():\n",
        "  '''\n",
        "  description_storage_path: .json file for output description\n",
        "  rag_storage_path: path for RAG database\n",
        "  progress_path: for resume the pipeline\n",
        "  '''\n",
        "  def __init__(self, description_storage_path, rag_storage_path,\n",
        "    model_name=\"models/gemma-3-27b-it\",\n",
        "    rate_limit=5\n",
        "  ):\n",
        "    self._lvm = LVM(model_name=model_name, rate_limit=rate_limit)\n",
        "    self._image_description_storage = ImageDescriptionStorage(path=description_storage_path)\n",
        "    self._rag = RAG(storage_path=rag_storage_path)\n",
        "\n",
        "    self._image_description_storage.load()\n",
        "\n",
        "  def append_to_description_file(self, image_dir: str, batch_size=100):\n",
        "    from PIL import Image\n",
        "    import os\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    all_filenames = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "    all_filenames.sort() # Sorting ensures consistent ordering across runs\n",
        "\n",
        "    # --- Resume Logic ---\n",
        "    # Get the set of filenames already processed\n",
        "    processed_filenames = {d['filename'] for d in self._image_description_storage.get_data()}\n",
        "\n",
        "    # Filter out already processed filenames\n",
        "    filenames_to_process = [f for f in all_filenames if f not in processed_filenames]\n",
        "    # --- End Resume Logic ---\n",
        "\n",
        "    print(f\"Found {len(all_filenames)} images in total.\")\n",
        "    print(f\"Skipping {len(processed_filenames)} already processed images.\")\n",
        "    print(f\"Processing {len(filenames_to_process)} new images.\")\n",
        "\n",
        "    num_to_process = len(filenames_to_process)\n",
        "    num_processed_this_run = 0\n",
        "\n",
        "    while num_processed_this_run < num_to_process:\n",
        "        start_index = num_processed_this_run\n",
        "        end_index = min(num_processed_this_run + batch_size, num_to_process)\n",
        "        batch_filenames = filenames_to_process[start_index:end_index]\n",
        "\n",
        "        images = []\n",
        "        for filename in tqdm(batch_filenames, desc=f\"Loading images for batch {num_processed_this_run // batch_size + 1}\"):\n",
        "            img_path = os.path.join(image_dir, filename)\n",
        "            try:\n",
        "                img = Image.open(img_path)\n",
        "                images.append(img)\n",
        "            except Exception as e:\n",
        "                print(f\"Error opening image {img_path}: {e}\")\n",
        "\n",
        "        # Check if any images were successfully loaded in the batch\n",
        "        if not images:\n",
        "            print(f\"No images successfully loaded in batch {num_processed_this_run // batch_size + 1}. Skipping VLM call.\")\n",
        "            # Important: Still need to advance the counter to avoid infinite loop if all images in batch fail\n",
        "            num_processed_this_run += len(batch_filenames)\n",
        "            continue # Skip to the next batch\n",
        "\n",
        "        print(f\"Processing batch {num_processed_this_run // batch_size + 1} with {len(images)} images.\")\n",
        "        descriptions = self._lvm.images_to_text(images)\n",
        "\n",
        "        for filename, description in zip(batch_filenames, descriptions):\n",
        "            self._image_description_storage.append({\n",
        "                'filename': filename,\n",
        "                'description': description\n",
        "            })\n",
        "        self._image_description_storage.save()\n",
        "\n",
        "        num_processed_this_run += len(batch_filenames)\n",
        "\n",
        "\n",
        "  def rag_indexing(self):\n",
        "    data = self._image_description_storage.load()\n",
        "    self._rag.index(\n",
        "      data=[d['description'] for d in data],\n",
        "      annotation=[d['filename'] for d in data]\n",
        "    )\n",
        "\n",
        "\n",
        "#---------------------------[Example usage]----------------------------------\n",
        "\n",
        "pipeline = Pipeline_GenerateDescriptions(\n",
        "    description_storage_path='./test_images_description.json',\n",
        "    rag_storage_path='./test_rag_data.json'\n",
        ")\n",
        "\n",
        "pipeline.append_to_description_file('./test_images_database')\n",
        "#get images_description.json file completed\n",
        "\n",
        "pipeline.rag_indexing()\n",
        "#rag is read for use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-9ugYjl7oAFF"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Input: 1 image\n",
        "Output: n-image similar to this image\n",
        "'''\n",
        "\n",
        "import PIL\n",
        "\n",
        "class Pipeline_RetrieveImages():\n",
        "  def __init__(self, rag_storage_path, image_dir):\n",
        "    self._lvm = LVM(model_name=\"models/gemma-3-27b-it\", rate_limit=5)\n",
        "    self._rag = RAG(storage_path=rag_storage_path)\n",
        "    self._image_dir = image_dir\n",
        "\n",
        "  def _rag_retrieve(self, image: PIL.Image, n=5):\n",
        "    description = self._lvm.images_to_text([image])[0]\n",
        "    answer = self._rag.query(query=description, top_k=n)\n",
        "    return {\n",
        "      'input_description': description,\n",
        "      'filenames': answer['annotation'],\n",
        "      'descriptions': answer['data']\n",
        "    }\n",
        "\n",
        "  def image_retrieve(self, image: PIL.Image, n=5):\n",
        "    import os\n",
        "    from PIL import Image\n",
        "\n",
        "    rag_retrieved = self._rag_retrieve(image, n=n)\n",
        "    images = [\n",
        "      Image.open(os.path.join(self._image_dir, fname))\n",
        "        for fname in rag_retrieved['filenames']\n",
        "    ]\n",
        "    return {\n",
        "      'input_description': rag_retrieved['input_description'],\n",
        "      'images': images,\n",
        "      'descriptions': rag_retrieved['descriptions']\n",
        "    }\n",
        "\n",
        "#-------------------------------[Example usage]--------------------------------\n",
        "pipeline = Pipeline_RetrieveImages(\n",
        "  rag_storage_path='./test_rag_data.json',\n",
        "  image_dir='./image_corpus'\n",
        ")\n",
        "rag_retrieved = pipeline.image_retrieve(mrag_bench[0]['image'], n=2)\n",
        "\n",
        "print('input image is')\n",
        "display(mrag_bench[0]['image'])\n",
        "print(rag_retrieved['input_description'])\n",
        "\n",
        "print('retrived image is')\n",
        "for img, desc in zip(rag_retrieved['images'], rag_retrieved['descriptions']):\n",
        "  display(img)\n",
        "  print(desc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xkSWd5-r7FyE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Given a question, an image, do\n",
        "\n",
        "1. get relevant images from RAG\n",
        "2. feed all images (input & from RAG) to LVM\n",
        "'''\n",
        "import PIL\n",
        "\n",
        "class Pipeline_GenerateAnswers():\n",
        "  def __init__(self, rag_storage_path, image_dir):\n",
        "    self._lvm = LVM(model_name=\"models/gemma-3-27b-it\", rate_limit=5)\n",
        "    self._pipeline_retrieve_image = Pipeline_RetrieveImages(\n",
        "      rag_storage_path=rag_storage_path,\n",
        "      image_dir=image_dir\n",
        "    )\n",
        "\n",
        "  def _get_answer(self, prompt: str, image: PIL.Image, n_retrieve_img=5):\n",
        "    rag_retrieved = self._pipeline_retrieve_image.image_retrieve(\n",
        "        image,\n",
        "        n=n_retrieve_img\n",
        "    )\n",
        "    return {\n",
        "      'rag_retrived': rag_retrieved,\n",
        "      'prompt': prompt,\n",
        "      'chat_result': self._lvm.multi_images_chat(\n",
        "          prompt=prompt,\n",
        "          images=[image]+rag_retrieved['images']\n",
        "      )\n",
        "    }\n",
        "\n",
        "  def get_answer_from_dataset(self, data_point, n_retrieve_img=5,\n",
        "    format_prompt='Strictly give your answer in format `<A/B/C/D> <Reason>`'\n",
        "  ):\n",
        "    question = data_point['question']\n",
        "    image = data_point['image']\n",
        "    choice_strg = f\"{format_prompt}\\nChoices are:\\nA:{data_point['A']}\\nB:{data_point['B']}\\nC:{data_point['C']}\\nD:{data_point['D']}\"\n",
        "\n",
        "    prompt = f\"{question}\\n{choice_strg}\"\n",
        "\n",
        "    return self._get_answer(prompt=prompt, image=image, n_retrieve_img=n_retrieve_img)\n",
        "\n",
        "#-------------------------------[Example usage]--------------------------------\n",
        "\n",
        "pipeline_answer = Pipeline_GenerateAnswers(\n",
        "  rag_storage_path='./test_rag_data.json',\n",
        "  image_dir='./image_corpus'\n",
        ")\n",
        "\n",
        "d = mrag_bench[0]\n",
        "display(d)\n",
        "\n",
        "lvm_ans = pipeline_answer.get_answer_from_dataset(data_point=d)\n",
        "display(lvm_ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XPkdeHqe9mF"
      },
      "outputs": [],
      "source": [
        "#-----------------------------------------[Main Code]---------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TBkQhDn7e5KM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "use the offically provided images as database images\n",
        "\n",
        "can be resumed from interrupte:\n",
        "  put images_description.json at ./\n",
        "  start the cell\n",
        "\n",
        "rag_data.json is required to get back the image description when doing query\n",
        "'''\n",
        "\n",
        "pipeline_description = Pipeline_GenerateDescriptions(\n",
        "    description_storage_path='./images_description.json',\n",
        "    rag_storage_path='./rag_data.json',\n",
        "    rate_limit=5\n",
        ")\n",
        "\n",
        "#generate description by calling LVM on each image files in this directory\n",
        "pipeline_description.append_to_description_file('./image_corpus')\n",
        "\n",
        "pipeline_description.rag_indexing()\n",
        "#rag is read for use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl5FIlz4d1mC"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "process the output from LVM to input format of evalting package.\n",
        "'''\n",
        "\n",
        "import shortuuid\n",
        "import json\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "pipeline_answer = Pipeline_GenerateAnswers(\n",
        "  rag_storage_path='./rag_data.json',\n",
        "  image_dir='./image_corpus'\n",
        ")\n",
        "\n",
        "answers=[]\n",
        "\n",
        "#display(mrag_bench)\n",
        "\n",
        "i=0\n",
        "for d in tqdm(mrag_bench):\n",
        "  i+=1\n",
        "  if i >= 20:\n",
        "    break\n",
        "\n",
        "  lvm_ans = pipeline_answer.get_answer_from_dataset(data_point=d)\n",
        "\n",
        "  answers.append({\n",
        "    \"qs_id\": d['id'],\n",
        "    \"prompt\": lvm_ans['prompt'],\n",
        "    \"output\": lvm_ans['chat_result'],\n",
        "    \"gt_answer\": d['answer'],\n",
        "    \"shortuuid\": shortuuid.uuid(),\n",
        "    \"model_id\": 'gemma-3-27b-it',\n",
        "    \"gt_choice\": d['answer_choice'],\n",
        "    \"scenario\": d['scenario'],\n",
        "    \"aspect\": d['aspect'],\n",
        "  })\n",
        "\n",
        "  #display(d['image'])\n",
        "  #for retrived_img, desc in zip(lvm_ans['rag_retrived']['images'],lvm_ans['rag_retrived']['descriptions']):\n",
        "  #  display(retrived_img)\n",
        "  #  print(desc)\n",
        "\n",
        "\n",
        "with open('answer.json', 'w') as ans_file:\n",
        "  json.dump(answers, ans_file, indent=4)\n",
        "  ans_file.flush()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbHAuu8oQ7Lb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Purge the answer output from LVM\n",
        "Otherwise, evalting package encouter error, or required to call openai api\n",
        "\n",
        "i.e.\n",
        " 1. Convert output string to Pure A/B/C/D\n",
        " 2. Use random choice if no answer(A/B/C/D) is found.\n",
        "'''\n",
        "def extract_choice_from_output(original_file, extracted_file):\n",
        "  import json\n",
        "  from tqdm import tqdm\n",
        "  import random\n",
        "\n",
        "  # Read data from answer.json\n",
        "  with open(original_file, 'r') as f:\n",
        "      answers = json.load(f)\n",
        "\n",
        "  # Process each entry\n",
        "  for entry in tqdm(answers):\n",
        "      if 'output' in entry and entry['output']: # Check if 'output' exists and is not empty\n",
        "          original_output = entry['output'].upper()\n",
        "          found_char = None\n",
        "          for char in original_output:\n",
        "              if 'A' <= char <= 'Z': # Check if the character is an uppercase letter\n",
        "                  found_char = char\n",
        "                  break # Stop searching once the first uppercase letter is found\n",
        "\n",
        "          if found_char:\n",
        "              entry['output'] = found_char\n",
        "          else:\n",
        "              entry['output'] = random.choice(['A', 'B', 'C', 'D']) + ' <random>'\n",
        "\n",
        "  # Save the modified data to a new file (or overwrite the original if you prefer)\n",
        "  with open(extracted_file, 'w') as f:\n",
        "      json.dump(answers, f, indent=4)\n",
        "\n",
        "extract_choice_from_output(\n",
        "  original_file='answer.json',\n",
        "  extracted_file='answer_purged.json'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_7ky21sr5e4"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mragbench/MRAG-Bench.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLamuhqj3dA0"
      },
      "outputs": [],
      "source": [
        "#use evalting package\n",
        "!python MRAG-Bench/eval/score.py -i \"answer_purged.json\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}